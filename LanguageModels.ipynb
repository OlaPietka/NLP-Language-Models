{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6da107d",
   "metadata": {},
   "source": [
    "# <font color='red'>Language Models</font>\n",
    "\n",
    "Here, I will train some <b>n-gram language models</b> on WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb087ce",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "To make models more robust, it is necessary to perform some basic preprocessing on the corpora.\n",
    "\n",
    "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp; We are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
    "\n",
    "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow models to generate sentences that have realistic beginnings and endings.\n",
    "\n",
    "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating the models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ccbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.12.1 which is incompatible.\n",
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\wiewi\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached torchdata-0.4.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\wiewi\\appdata\\roaming\\python\\python37\\site-packages (from torchdata) (2.24.0)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in c:\\users\\wiewi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torchdata) (2.5.1)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp37-cp37m-win_amd64.whl (161.9 MB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\wiewi\\appdata\\roaming\\python\\python37\\site-packages (from torchdata) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wiewi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->torchdata) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wiewi\\appdata\\roaming\\python\\python37\\site-packages (from requests->torchdata) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\wiewi\\appdata\\roaming\\python\\python37\\site-packages (from requests->torchdata) (3.0.4)\n",
      "Requirement already satisfied: pywin32>=226; platform_system == \"Windows\" in c:\\users\\wiewi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from portalocker>=2.0.0->torchdata) (300)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wiewi\\appdata\\roaming\\python\\python37\\site-packages (from torch==1.12.1->torchdata) (3.7.4.3)\n",
      "Installing collected packages: torch, torchdata\n",
      "Successfully installed torch-1.12.1 torchdata-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"<s>\"   # Start-of-sentence token\n",
    "END = \"</s>\"    # End-of-sentence-token\n",
    "UNK = \"<UNK>\"   # Unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cd58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "\n",
    "def preprocess(data, vocab=None):\n",
    "    final_data = []\n",
    "    lowercase = string.ascii_lowercase\n",
    "    \n",
    "    for paragraph in data:\n",
    "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
    "        \n",
    "        if vocab is not None:\n",
    "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
    "            \n",
    "        if paragraph == [] or paragraph.count('=') >= 2: \n",
    "            continue\n",
    "            \n",
    "        sen = []\n",
    "        prev_punct, prev_quot = False, False\n",
    "        \n",
    "        for word in paragraph:\n",
    "            if prev_quot:\n",
    "                if word[0] not in lowercase:\n",
    "                    final_data.append(sen)\n",
    "                    sen = []\n",
    "                    prev_punct, prev_quot = False, False\n",
    "            if prev_punct:\n",
    "                if word == '\"':\n",
    "                    prev_punct, prev_quot = False, True\n",
    "                else:\n",
    "                    if word[0] not in lowercase:\n",
    "                        final_data.append(sen)\n",
    "                        sen = []\n",
    "                        prev_punct, prev_quot = False, False\n",
    "                        \n",
    "            if word in {'.', '?', '!'}: \n",
    "                prev_punct = True\n",
    "            sen += [word]\n",
    "            \n",
    "        if sen[-1] not in {'.', '?', '!', '\"'}: \n",
    "            continue # Prevent a lot of short sentences\n",
    "        final_data.append(sen)\n",
    "        \n",
    "    vocab_was_none = vocab is None\n",
    "    \n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        \n",
    "    for i in range(len(final_data)):\n",
    "        final_data[i] = [START] + final_data[i] + [END]\n",
    "        \n",
    "        if vocab_was_none:\n",
    "            for word in final_data[i]:\n",
    "                vocab.add(word)\n",
    "    return final_data, vocab\n",
    "\n",
    "def getDataset():\n",
    "    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))\n",
    "    train_dataset, vocab = preprocess(dataset[0])\n",
    "    test_dataset, _ = preprocess(dataset[1], vocab)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = getDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9d9cf",
   "metadata": {},
   "source": [
    "Lets see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0364d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', '</s>']\n",
      "['<s>', 'The', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'May', \"'n\", '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for sentance in train_dataset[7:9]:\n",
    "    print(sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4861bb6",
   "metadata": {},
   "source": [
    "## The LanguageModel Base Class\n",
    "\n",
    "I will implement 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model.\n",
    "\n",
    "* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model implementing.\n",
    "\n",
    "* <b>`generateSentence(self)`</b>: Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in the vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). I assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to language model's distribution. The number of words <TT>n</TT> is not fixed; instead, I stop the sentence as soon as I generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n",
    "\n",
    "* <b>`getSentenceLogProbability(self, sentence)`</b>: Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>.\n",
    "\n",
    "* <b>`getCorpusPerplexity(self, testCorpus)`</b>: Compute the perplexity (normalized inverse log probability) of `testCorpus` according to the model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells us to compute perplexity as follows: \n",
    "\n",
    "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
    "\n",
    "In order to avoid underflow, I will need to do all of my calculations in log-space. That is, instead of multiplying probabilities, I add the logarithms of the probabilities and exponentiate the result:\n",
    "\n",
    "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f619b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class LanguageModel(object):\n",
    "    def __init__(self, train_corpus):\n",
    "        '''\n",
    "        Initialize and train the model \n",
    "        '''\n",
    "        return\n",
    "\n",
    "    def generate_sentence(self):\n",
    "        '''\n",
    "        Generate a sentence by drawing words according to the model's probability distribution.\n",
    "        '''\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence):\n",
    "        '''\n",
    "        Calculate the log probability of the sentence provided. \n",
    "        '''\n",
    "\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_corpus_perplexity(self, test_corpus):\n",
    "        '''\n",
    "        Calculate the perplexity of the corpus provided.\n",
    "        '''\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def print_sentences(self, n):\n",
    "        '''\n",
    "        Prints n sentences generated by model.\n",
    "        '''\n",
    "\n",
    "        for i in range(n):\n",
    "            sent = self.generate_sentence()\n",
    "            prob = self.get_sentence_log_probability(sent)\n",
    "            print('Log Probability:', prob , '\\tSentence:',sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d46415",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='green'>Unigram Model</font>\n",
    "\n",
    "The probability distribution of a word is given by $\\hat P(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43b9a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counter(dictionary, word):\n",
    "    if word in dictionary.keys():\n",
    "        dictionary[word] += 1\n",
    "    else:\n",
    "        dictionary[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcf02064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class UnigramModel(LanguageModel):\n",
    "    def __init__(self, train_corpus):\n",
    "        self.train_corpus = train_corpus\n",
    "        self.P = self.get_tokens_prob()\n",
    "\n",
    "    def get_tokens_prob(self):\n",
    "        unigram_count = {}\n",
    "        \n",
    "        for sentence in self.train_corpus:\n",
    "            for word in sentence:\n",
    "                if word == START:\n",
    "                    continue\n",
    "                    \n",
    "                unigram_counter(unigram_count, word)\n",
    "        \n",
    "        N = sum(unigram_count.values())\n",
    "        return {k: v / N for k, v in unigram_count.items()}\n",
    " \n",
    "    def generate_sentence(self):\n",
    "        sentence = [START]\n",
    "\n",
    "        word = ''\n",
    "        while word != END:\n",
    "            word = choices(list(self.P.keys()), list(self.P.values()))[0]\n",
    "            sentence.append(word)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence):\n",
    "        sentence_prob = [self.P[word] for word in sentence[1:]]\n",
    "        return np.sum(np.log(sentence_prob))\n",
    "        \n",
    "    def get_corpus_perplexity(self, test_corpus):\n",
    "        logP = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            N += len(sentence) - 1\n",
    "            logP += self.get_sentence_log_probability(sentence)\n",
    "           \n",
    "        perplexity = math.exp(-logP / N)\n",
    "        return perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e9cb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(ModelObject):\n",
    "    model = ModelObject(train_dataset)\n",
    "   \n",
    "    print(\"--------- 5 sentences from the model ---------\")\n",
    "    model.print_sentences(5)\n",
    "\n",
    "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
    "    print (\"Training Set:\", model.get_corpus_perplexity(train_dataset))\n",
    "    print (\"Testing Set:\", model.get_corpus_perplexity(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "882ffaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from the model ---------\n",
      "Log Probability: -488.8695654861011 \tSentence: ['<s>', 'they', 'Even', 'group', 'six', '<UNK>', 'present', 'poor', '1912', 'topped', 'Scots', 'the', '@-@', '<UNK>', 'shot', 'also', 'they', 'from', 'commanded', 'high', '(', 'could', 'time', 'war', 'northern', ',', '.', 'She', 'Bracknell', 'overt', 'to', 'behind', 'pitch', 'until', 'about', 'press', '2', ')', 'central', '1933', 'Bob', 'or', 'the', 'Jacob', '<UNK>', ',', 'atmosphere', ',', \"'s\", 'still', 'his', 'Marines', '1', 'following', 'just', 'the', 'for', '.', 'his', 'to', '.', 'brief', 'there', 'was', 'totals', 'career', 'during', 'the', 'politicians', '</s>']\n",
      "Log Probability: -137.7611746418094 \tSentence: ['<s>', 'and', 'long', 'war', 'What', 'of', 'sales', 'not', ',', '8', 'the', 'called', 'hydrophobic', 'decided', 'the', '<UNK>', ',', 'of', '.', 'created', 'has', 'runway', '</s>']\n",
      "Log Probability: -222.0862373636816 \tSentence: ['<s>', 'Sweetums', 'decided', 'M', 'by', '.', 'them', 'that', 'duo', 'distribution', 'abandon', 'Bible', 'in', 'twelve', 'there', 'testing', 'pagoda', 'to', 'Carolina', 'route', 'acquired', ')', '\"', '.', 'are', 'the', '<UNK>', 'states', 'in', 'featured', '</s>']\n",
      "Log Probability: -283.94046385042606 \tSentence: ['<s>', '<UNK>', 'a', 'groups', 'hunting', '(', 'by', ',', 'A', 'and', 'the', 'in', 'was', 'O', ',', 'the', 'gave', 'images', 'interpret', 'during', 'left', 'a', 'not', 'He', 'in', 'the', ',', 'entered', '@.@', 'The', 'The', 'over', 'she', 'the', 'extended', 'tribute', 'Michelin', 'and', 'when', 'beginning', '@-@', 'openings', 'gritty', 'who', '</s>']\n",
      "Log Probability: -32.999051995931964 \tSentence: ['<s>', 'Edward', 'ignored', 'Other', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 1101.9435880266938\n",
      "Testing Set: 912.157438593044\n"
     ]
    }
   ],
   "source": [
    "run(UnigramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0baefb",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='green'>Smoothed Unigram Model</font>\n",
    "\n",
    "Here, I will implement each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
    "\n",
    "If $c(w)$ is the frequency of $w$ in the training data, we can compute $P_L(w)$ as follows:\n",
    "\n",
    "$$P_L(w)=\\frac{c(w)+1}{N+S}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87c850eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedUnigramModel(UnigramModel):\n",
    "    def __init__(self, train_corpus):\n",
    "        super().__init__(train_corpus)\n",
    "        self.P = self.get_tokens_prob()\n",
    "        \n",
    "    def get_tokens_prob(self):\n",
    "        unigram_count = {}\n",
    "        \n",
    "        for sentence in self.train_corpus:\n",
    "            for word in sentence:\n",
    "                if word == START:\n",
    "                    continue\n",
    "                    \n",
    "                unigram_counter(unigram_count, word)\n",
    "        \n",
    "        N = sum(unigram_count.values())\n",
    "        S = len(unigram_count.keys())\n",
    "        return {k: (v + 1) / (N + S) for k, v in unigram_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f456c607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from the model ---------\n",
      "Log Probability: -350.63440837925555 \tSentence: ['<s>', 'performed', 'AI', 'neck', 'to', 'without', ',', '.', 'to', 'of', 'Expeditionary', 'the', 'the', 'freeway', 'was', '1990', 'risk', 'of', 'Mississippi', 'and', 'and', 'Kenna', 'BC', ',', 'who', 'the', '.', 'severely', 'schools', 'move', '.', '@-@', 'of', 'head', 'people', 'the', 'DVD', 'for', 'single', 'built', 'reach', 'the', 'perfect', 'until', 'effect', 'out', 'written', 'closed', 'King', 'several', 'future', '</s>']\n",
      "Log Probability: -128.30731509747412 \tSentence: ['<s>', 'White', 'a', 'she', 'model', 'may', 'This', 'the', 'that', 'Torres', 'with', 'of', 'Age', '<UNK>', 'Other', 'renting', 'a', '49', 'to', '</s>']\n",
      "Log Probability: -135.5298851522499 \tSentence: ['<s>', '.', 'album', 'forums', 'He', 'run', 'students', 'benign', 'struck', 'and', 'saw', ',', 'the', 'which', 'Paley', 'Battery', '<UNK>', 'prominent', '</s>']\n",
      "Log Probability: -176.0977621206662 \tSentence: ['<s>', 'to', '\"', 'then', ',', 'pieces', 'saw', 'the', 'star', 'Wanna', 'BRIT', 'pair', 'Johannes', 'to', 'by', 'Reef', '37', 'Ransome', 'His', 'caused', 'and', 'the', 'whose', '</s>']\n",
      "Log Probability: -169.6710861771229 \tSentence: ['<s>', 'difference', 'and', ',', 'their', '.', 'with', 'and', 'Kinda', 'with', ',', 'that', 'residential', 'His', 'respond', 'awarded', 'live', 'describes', 'forces', ',', 'conservation', 'points', 'in', 'remembering', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 1103.0243317444865\n",
      "Testing Set: 914.472450228316\n"
     ]
    }
   ],
   "source": [
    "run(SmoothedUnigramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406394e",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='green'>Bigram Model</font>\n",
    "\n",
    "Here, I will implement each of the 4 functions described above for an <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa7e6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counter(dictionary, word1, word2):\n",
    "    if word1 in dictionary.keys():\n",
    "        if word2 in dictionary[word1].keys():\n",
    "            dictionary[word1][word2] += 1\n",
    "        else:\n",
    "            dictionary[word1][word2] = 1\n",
    "    else:\n",
    "        dictionary[word1] = {word2: 1}\n",
    "        \n",
    "def bigram_assigner(dictionary, word1, word2, value):\n",
    "    if word1 in dictionary.keys():\n",
    "        if word2 not in dictionary[word1].keys():\n",
    "            dictionary[word1][word2] = value\n",
    "    else:\n",
    "        dictionary[word1] = {word2: value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1fa3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(LanguageModel):\n",
    "    def __init__(self, train_corpus):\n",
    "        self.train_corpus = train_corpus\n",
    "        self.P = self.get_tokens_prob()\n",
    "            \n",
    "    def get_tokens_prob(self):\n",
    "        unigram_count = {}\n",
    "        bigram_count = {}\n",
    "\n",
    "        for sentence in self.train_corpus:\n",
    "            for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "                unigram_counter(unigram_count, w1)\n",
    "                bigram_counter(bigram_count, w1, w2)\n",
    "        \n",
    "        P = {}\n",
    "        for w1, dic in bigram_count.items():\n",
    "            for w2, count in dic.items():\n",
    "                prob = count / unigram_count[w1]\n",
    "                bigram_assigner(P, w1, w2, prob)\n",
    "        return P\n",
    "\n",
    "    def generate_sentence(self):\n",
    "        sentence = []\n",
    "        w1 = START\n",
    "        w2 = None\n",
    "\n",
    "        while w2 != END:\n",
    "            w2 = choices(list(self.P[w1].keys()), list(self.P[w1].values()))[0]\n",
    "            sentence.append(w1)\n",
    "            w1 = w2\n",
    "            \n",
    "        sentence.append(w2)\n",
    "        return sentence\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence):\n",
    "        sentence_sumlogP = 0\n",
    "        \n",
    "        for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "            if w1 in self.P.keys():\n",
    "                if w2 in self.P[w1].keys():\n",
    "                    sentence_sumlogP += np.log(self.P[w1][w2])\n",
    "                    continue\n",
    "            sentence_sumlogP += np.log(0.0)\n",
    "                \n",
    "        return sentence_sumlogP\n",
    "        \n",
    "    def get_corpus_perplexity(self, test_corpus):\n",
    "        logP = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            N += len(sentence) - 1\n",
    "            logP += self.get_sentence_log_probability(sentence)\n",
    "\n",
    "        perplexity = math.exp(-logP / N)\n",
    "        return perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cdc885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from the model ---------\n",
      "Log Probability: -51.31658879665059 \tSentence: ['<s>', 'The', 'Early', 'versions', ';', 'its', 'duration', 'meant', 'to', 'be', 'an', 'eye', '.', '</s>']\n",
      "Log Probability: -71.59756425498249 \tSentence: ['<s>', 'It', 'was', 'involved', 'in', 'North', 'Carolina', 'to', 'open', 'and', 'that', 'the', 'counties', ',', 'and', 'face', '.', '\"', '</s>']\n",
      "Log Probability: -34.579534101736876 \tSentence: ['<s>', 'The', 'New', 'York', 'Zoological', 'Nomenclature', ',', 'it', 'was', 'not', 'use', '.', '</s>']\n",
      "Log Probability: -70.30705398288661 \tSentence: ['<s>', 'Walker', 'taking', 'the', 'predecessor', ',', 'Denise', '(', '910', 'Arabs', 'of', 'other', 'time', '.', '</s>']\n",
      "Log Probability: -128.58157795678525 \tSentence: ['<s>', 'Djedkare', \"'s\", 'residence', 'of', 'the', 'rising', 'to', 'coincide', 'with', 'his', 'head', 'of', 'her', 'that', 'connects', 'to', 'yield', '(', 'Corey', 'Moss', 'said', 'the', 'centenary', 'events', 'in', 'an', 'attack', '.', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 76.92394608735728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wiewi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set: inf\n"
     ]
    }
   ],
   "source": [
    "run(BigramModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482221b",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='green'>Smoothed Bigram Model</font>\n",
    "\n",
    "Here, I will implement each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n",
    "\n",
    "In order to smooth our model, I need to compute a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, we can compute $D$ as: \n",
    "\n",
    "$$D=\\frac{n_1}{n_1+2n_2}$$ \n",
    "\n",
    "For each word $w$, I then need to compute the number of bigram types $ww’$ as follows: \n",
    "\n",
    "$$S(w)=|\\{w’\\mid c(ww’)>0\\}|$$ \n",
    "\n",
    "where $c(ww’)$ is the frequency of $ww’$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n",
    "\n",
    "Finally, I compute $P_{AD}(w’|w)$ as follows: \n",
    "\n",
    "$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$ \n",
    "\n",
    "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b0d6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedBigramModelAD(BigramModel):\n",
    "    def __init__(self, train_corpus):\n",
    "        self.train_corpus = train_corpus\n",
    "        self.D, self.N, self.S = 0, 0, 0\n",
    "        self.bigram_count, self.unigram_count = {}, {}\n",
    "        self.P = self.get_tokens_prob()\n",
    "\n",
    "    def nk(self, bigram_count, k):\n",
    "        return sum([1 for w1, dic in bigram_count.items() for w2, count in dic.items() if count == k])\n",
    "    \n",
    "                    \n",
    "    def P_AD(self, w1, w2):\n",
    "        S_w = len(self.bigram_count[w1].keys())\n",
    "        c_ww = self.get_bigram_count(w1, w2)\n",
    "        c_w = self.get_unigram_count(w1)\n",
    "        P_L = (self.unigram_count[w2] + 1) / (self.N + self.S)\n",
    "        \n",
    "        return (max(c_ww - self.D, 0) / c_w) + (self.D / c_w * S_w * P_L)\n",
    "    \n",
    "    def get_unigram_count(self, word):\n",
    "        if word in self.bigram_count.keys():\n",
    "            return self.unigram_count[word]\n",
    "        return 0\n",
    "            \n",
    "    def get_bigram_count(self, w1, w2):\n",
    "        if w1 in self.bigram_count.keys():\n",
    "            if w2 in self.bigram_count[w1].keys():\n",
    "                return self.bigram_count[w1][w2]\n",
    "        return 0\n",
    "\n",
    "    def get_tokens_prob(self):\n",
    "        for sentence in self.train_corpus:\n",
    "            unigram_counter(self.unigram_count, sentence[0])\n",
    "                    \n",
    "            for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "                unigram_counter(self.unigram_count, w2)\n",
    "                bigram_counter(self.bigram_count, w1, w2)\n",
    "               \n",
    "        n1, n2 = self.nk(self.bigram_count, 1), self.nk(self.bigram_count, 2)\n",
    "        self.D = n1 / (n1 + 2 * n2)\n",
    "        \n",
    "        self.N = sum([c for w, c in self.unigram_count.items() if w != START])\n",
    "        self.S = len([w for w, c in self.unigram_count.items() if w != START])\n",
    "        \n",
    "        P = {}\n",
    "        for sentence in self.train_corpus:\n",
    "            for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "                prob = self.P_AD(w1, w2)\n",
    "                bigram_assigner(P, w1, w2, prob)\n",
    "        return P\n",
    "    \n",
    "    def get_sentence_log_probability(self, sentence):\n",
    "        sentence_sumlogP = 0\n",
    "        \n",
    "        for w1, w2 in zip(sentence[:-1], sentence[1:]):\n",
    "            if w1 in self.P.keys():\n",
    "                if w2 in self.P[w1].keys():\n",
    "                    sentence_sumlogP += np.log(self.P[w1][w2])\n",
    "                    continue\n",
    "                    \n",
    "            prob = self.P_AD(w1, w2)\n",
    "            sentence_sumlogP += np.log(prob)\n",
    "                \n",
    "        return sentence_sumlogP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "018c8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- 5 sentences from the model ---------\n",
      "Log Probability: -78.30308712471995 \tSentence: ['<s>', 'With', 'the', 'greatest', 'threat', '...', 'I', 'have', 'to', 'Yankovic', 'began', 'using', 'any', 'of', 'them', 'into', 'disgrace', '.', '</s>']\n",
      "Log Probability: -35.078650995464216 \tSentence: ['<s>', 'He', 'returned', 'to', 'his', 'second', 'season', ',', 'describing', 'it', '.', '</s>']\n",
      "Log Probability: -87.20450298595823 \tSentence: ['<s>', 'This', 'gave', 'the', '<UNK>', 'back', ',', 'focusing', 'on', 'that', 'it', \"'s\", 'coffin', ',', 'Dahau', 'and', 'Kauffman', 'themselves', '.', '</s>']\n",
      "Log Probability: -198.70607196103828 \tSentence: ['<s>', 'By', 'June', ',', 'with', 'the', 'January', '1', 'draw', 'at', 'the', 'cemetery', 'was', 'widespread', ',', 'accidents', 'in', '1987', 'retrospective', 'of', 'Brazil', 'Luiz', '<UNK>', 'in', 'a', 'total', 'of', 'northern', 'China', 'have', 'basis', 'in', 'two', 'greatest', 'hits', ',', 'and', 'non', '@-@', 'down', 'and', 'possessions', '.', '</s>']\n",
      "Log Probability: -187.112550030754 \tSentence: ['<s>', 'The', 'Undertaker', ',', 'and', 'The', 'male', 'puma', ',', 'though', 'six', 'spots', ',', 'of', 'the', 'most', 'valuable', 'players', 'select', 'a', 'large', 'dinosaur', ',', 'in', 'its', 'return', 'the', 'moon', ',', 'during', 'the', 'Queensland', ',', 'while', 'his', 'reign', 'of', 'Common', 'starlings', 'include', '<UNK>', '.', '</s>']\n",
      "\n",
      "--------- Corpus Perplexities ---------\n",
      "Training Set: 98.5581292053259\n",
      "Testing Set: 272.57201979320354\n"
     ]
    }
   ],
   "source": [
    "run(SmoothedBigramModelAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27572a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
